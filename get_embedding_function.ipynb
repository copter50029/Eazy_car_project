{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f48399",
   "metadata": {},
   "source": [
    "# Ingesting PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036b3f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.1.1\n",
      "[notice] To update, run: d:\\Data_sci_project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.1.1\n",
      "[notice] To update, run: d:\\Data_sci_project\\.venv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --q unstructured langchain langchain-community\n",
    "%pip install --q \"unstructured[all-docs]\" ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d2fa582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\\EazyCar_Product_knowledge_for_AI_chatbot.pdf\"\n",
    "loader = UnstructuredPDFLoader(path)\n",
    "docs = loader.load()\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25952ad3",
   "metadata": {},
   "source": [
    "# Vecter Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4e1ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     ID              SIZE      MODIFIED       \n",
      "deepseek-r1:1.5b         e0979632db5a    1.1 GB    14 minutes ago    \n",
      "nomic-embed-text:v1.5    0a109f422b47    274 MB    2 days ago        \n"
     ]
    }
   ],
   "source": [
    "# !ollama pull nomic-embed-text:v1.5\n",
    "!ollama list\n",
    "# !pip install -q chromadb\n",
    "# !pip install -q langchain-text-splitters\n",
    "# !ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2088285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Split the documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "chucks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c69dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVI03\\AppData\\Local\\Temp\\ipykernel_18084\\2536676596.py:4: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embedding=OllamaEmbeddings(model=\"nomic-embed-text:v1.5\",show_progress=True),\n",
      "OllamaEmbeddings: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
     ]
    }
   ],
   "source": [
    "#Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chucks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text:v1.5\",show_progress=True),\n",
    "    collection_name=\"Eazycar_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edb61fc",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc3709c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0efed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AVI03\\AppData\\Local\\Temp\\ipykernel_18084\\1972936892.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_model)\n"
     ]
    }
   ],
   "source": [
    "local_model = \"deepseek-r1:1.5b\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "QUERRY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48ebca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(),\n",
    "    llm,\n",
    "    prompt=QUERRY_PROMPT\n",
    ")\n",
    "\n",
    "#RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\" \n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71734334",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2288e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<think>\\nFirst, I need to determine whether Eazy Car is a subscription service that offers cars or if it's about taxes. The context provided includes information on what Eazy Car is and how it handles insurance, taxes, and vehicle ownership. It mentions that Eazy Car provides flexible terms with a fixed monthly rate for all types of vehicles, including private and corporate use. It also covers how to apply for the service, the required documents, payment methods, security deposits, tax deductions, and cancellation procedures. The information is structured in several parts, each focusing on different aspects like service types, pricing, application, documentation, insurance, taxes, maintenance, and customer inquiries.\\n\\nSince Eazy Car focuses on offering cars as part of a subscription service with various terms and conditions related to payment, security, taxes, and service specifics, the primary focus is on its car offerings and management rather than general tax or insurance information. Therefore, I will conclude that the document discusses specific details about Eazy Car regarding car services and financial aspects but does not cover general tax laws or principles.\\n</think>\\n\\nEazy Car คือ什么呢?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e8705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
